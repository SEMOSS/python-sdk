{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Server SDK for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade ai-server-sdk\n",
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server Connection & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Server Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Am I connected to the server? True\n",
      "This is my current insight ID: 9e32cfb7-9474-489f-81a7-d00acf4930db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ai_server import ServerClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "SECRET_KEY = os.getenv('DEV_SECRET_KEY')\n",
    "ACCESS_KEY = os.getenv('DEV_ACCESS_KEY')\n",
    "\n",
    "# Here we connect to the demo instance of CFG, but you would connect to your own instance\n",
    "connection_url = os.getenv('DEV_ENDPOINT')\n",
    "\n",
    "# This object creates a connection to the CFG server\n",
    "server_connection = ServerClient(base = connection_url, access_key = ACCESS_KEY, secret_key = SECRET_KEY)\n",
    "\n",
    "# Check if you are connected to the server\n",
    "is_connected = server_connection.connected\n",
    "print(f\"Am I connected to the server? {is_connected}\")\n",
    "\n",
    "# Initilizing the server creates a new insight which is accessible through .cur_insight\n",
    "my_insight = server_connection.cur_insight\n",
    "print(f\"This is my current insight ID: {my_insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My current insight is now: 89826093-2c63-48e5-895e-f794085ef4e3\n"
     ]
    }
   ],
   "source": [
    "# You can create new insights by calling the .make_new_insight() method\n",
    "new_insight = server_connection.make_new_insight() # This becomes your active insight\n",
    "\n",
    "print(f\"My current insight is now: {server_connection.cur_insight}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing open insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are my open insights: ['9e32cfb7-9474-489f-81a7-d00acf4930db', '89826093-2c63-48e5-895e-f794085ef4e3']\n"
     ]
    }
   ],
   "source": [
    "my_open_insights = server_connection.get_open_insights()\n",
    "\n",
    "print(f\"These are my open insights: {my_open_insights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping insights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are my insight IDs after dropping: ['e69849f5-8956-494b-b6ce-596e7d6259c4']\n"
     ]
    }
   ],
   "source": [
    "# You can drop all of your insights with the .drop_insights() method and passing in the list of insights\n",
    "# Here we drop all of our open insight ids \n",
    "server_connection.drop_insights(my_open_insights) # This will create a new insight id since there must be an active insight\n",
    "\n",
    "print(f\"Here are my insight IDs after dropping: {server_connection.get_open_insights()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python trick to check attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the attributes of the server connection object: ['access_key', 'auth_headers', 'bearer_token', 'bearer_token_provider', 'connected', 'cookies', 'cur_insight', 'da_server', 'drop_insights', 'get_auth_headers', 'get_open_insights', 'get_openai_endpoint', 'get_partial_responses', 'get_pixel_output', 'import_data_product', 'loginBearerToken', 'loginUserAccessKey', 'logout', 'main_url', 'make_new_insight', 'monitors', 'open_insights', 'r', 'run_pixel', 'run_pixel_separate_thread', 'secret_key', 'send_request', 'upload_files']\n"
     ]
    }
   ],
   "source": [
    "attributes = dir(server_connection)\n",
    "# Find all of the attributes available to the server connection object\n",
    "attributes = [attr for attr in attributes if not attr.startswith('__')]\n",
    "\n",
    "print(f\"Here are the attributes of the server connection object: {attributes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both a pound of feathers and a pound of bricks weigh one pound. The difference lies in their density and volume, not their weight.\n",
      "We also get acess to: \n",
      "messageId: 8eacdf5b-ff13-48d8-b693-dff01471949d\n",
      "roomId: e69849f5-8956-494b-b6ce-596e7d6259c4\n",
      "numberOfTokensInPrompt 12\n",
      "numberOfTokensInResponse 24\n"
     ]
    }
   ],
   "source": [
    "from ai_server import ModelEngine\n",
    "\n",
    "# An ID for the specific LLM you want to interact with\n",
    "engine_id = os.getenv('DEV_LLM_CHAT_ENGINE_ID')\n",
    "\n",
    "# We need to pass the engine id and an insight id to the ModelEngine object\n",
    "llama3_model = ModelEngine(engine_id=engine_id, insight_id = server_connection.cur_insight)\n",
    "\n",
    "question = \"What weighs more, a pound of feathers or a pound of bricks?\"\n",
    "\n",
    "# The ask method sends a question to the model and returns the response\n",
    "try:\n",
    "    answer = llama3_model.ask(question)\n",
    "except Exception as e:\n",
    "    print(f\"An error occured: {e}\")\n",
    "\n",
    "print(answer['response'])\n",
    "\n",
    "print(\"We also get acess to: \")\n",
    "print(f\"messageId: {answer['messageId']}\")\n",
    "print(f\"roomId: {answer['roomId']}\")\n",
    "print(f\"numberOfTokensInPrompt\", answer['numberOfTokensInPrompt'])\n",
    "print(f\"numberOfTokensInResponse\", answer['numberOfTokensInResponse'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Inferencing with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We already talked about this. A pound of feathers or a pound of bricks both weigh the same - ONE POUND. They are like twins, they are the same weight!\n"
     ]
    }
   ],
   "source": [
    "# We can add parameters to our requests such as context, history, max_new_tokens, repetition_penalty, seed, temperature, top_k, top_p, truncate, typical_p\n",
    "params = {\n",
    "    \"temperature\": 0.9, \n",
    "    \"max_new_tokens\": 200,\n",
    "    \"context\": \"You are a first grade teacher that uses language and explanations easy for children to understand.\"\n",
    "    }\n",
    "\n",
    "# Pass your parameters as a dictionary to the ask method using the param_dict argument\n",
    "new_answer = llama3_model.ask(question, param_dict=params)\n",
    "\n",
    "print(new_answer['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's dive deeper into the concept of density and how it relates to mass and volume.\n",
      "\n",
      "Density is a fundamental property of matter that describes how much mass is packed into a given volume of a substance. It's calculated by dividing the mass of an object by its volume (density = mass/volume). The density of an object is typically expressed in units of mass per unit volume, such as grams per cubic centimeter (g/cm³) or kilograms per cubic meter (kg/m³).\n",
      "\n",
      "When comparing a pound of feathers to a pound of bricks, we can see that they have the same mass (1 pound), but their volumes are vastly different. A pound of feathers occupies a much larger volume than a pound of bricks due to its low density.\n",
      "\n",
      "To illustrate this point, consider a liter (or a cubic decimeter, dm³) of each substance. A liter of feathers would weigh about 0.036 pounds, while a liter of lead would weigh approximately 1.06 pounds (or 1 kilogram). This means that a liter of lead has a much higher density than a liter of feathers.\n",
      "\n",
      "Now, when we compare a pound of feathers to a pound of bricks, we're essentially comparing objects with the same mass but different volumes and densities. The key insight is that their masses are the same, which is what matters when we weigh them on a scale.\n",
      "\n",
      "In other words, a pound of feathers and a pound of bricks will always balance each other perfectly on a scale because they have the same mass, even though their densities and volumes differ significantly.\n",
      "\n",
      "This concept is crucial in understanding various phenomena in physics, engineering, and everyday life, such as buoyancy, fluid dynamics, and structural analysis. By recognizing the distinction between mass and density, we can make more informed decisions and develop a deeper appreciation for the intricate relationships between physical properties.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We can pass our own history to the ask method\n",
    "# You can use as many dictionaries as you want in the history list but each one will add tokens to the request\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": question},\n",
    "    {\"role\": \"assistant\", \"content\": new_answer['response']}\n",
    "]\n",
    "\n",
    "# Here we change the context to a college professor and pass our chat history\n",
    "params = {\n",
    "    \"temperature\": 0.9, \n",
    "    \"max_new_tokens\": 200,\n",
    "    \"context\": \"You are college professor who provides complex answers backed by science.\",\n",
    "    \"history\": history\n",
    "}\n",
    "\n",
    "# Our new question references the chat history\n",
    "new_question = \"Can you explain your previous answer in more detail?\"\n",
    "\n",
    "new_answer = llama3_model.ask(new_question, params)\n",
    "\n",
    "print(new_answer['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VectorEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ai_server import VectorEngine\n",
    "\n",
    "# Using a FAISS vector engine\n",
    "vector_engine_id = os.getenv('DEV_VECTOR_ENGINE_ID')\n",
    "\n",
    "# We initialize the VectorEngine object with the engine id and the current insight id\n",
    "faiss_vector_engine = VectorEngine(engine_id=vector_engine_id, insight_id=server_connection.cur_insight)\n",
    "\n",
    "file_path = \"./ai-whitehouse.pdf\"\n",
    "\n",
    "# We can add documents to the faiss index with the addDocument method\n",
    "faiss_vector_engine.addDocument(file_paths = [file_path])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing uploaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fileName': 'constitution.pdf', 'fileSize': 404.2470703125, 'lastModified': '2025-03-21 16:18:35'}, {'fileName': 'ai-whitehouse.pdf', 'fileSize': 578.16015625, 'lastModified': '2025-03-25 18:07:39'}]\n"
     ]
    }
   ],
   "source": [
    "# Fetch a list of uploaded documents\n",
    "my_documents = faiss_vector_engine.listDocuments()\n",
    "\n",
    "print(my_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a nearest neighbor search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result 1\n",
      "SCORE: 0.7842901945114136\n",
      "TOKENS: 165\n",
      "CONTENT: information, such as images, videos, audio clips, and text, that has been significantly modified or generated by algorithms, including by AI. (ff ) The term “testbed” means a facility or mechanism equipped for conducting rigorous, transparent, and replicable testing of tools and 11/15/23, 10:36 AM Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence | The White House https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-u… 10/63 technologies, including AI and PETs, to help evaluate the functionality, usability, and performance of those tools or technologies.\n",
      "Result 2\n",
      "SCORE: 0.7212790846824646\n",
      "TOKENS: 164\n",
      "CONTENT: of Artificial Intelligence | The White House https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-us… 6/63 manner; and use model inference to formulate options for information or action. (c) The term “AI model” means a component of an information system that implements AI technology and uses computational, statistical, or machine-learning techniques to produce outputs from a given set of inputs. (d) The term “AI red-teaming” means a structured testing effort to find flaws and vulnerabilities in an AI system, often in a controlled environment and in collaboration with developers of AI. Artificial Intelligence red- teaming\n",
      "Result 3\n",
      "SCORE: 0.6118261814117432\n",
      "TOKENS: 140\n",
      "CONTENT: term “Intelligence Community” has the meaning given to that term in section 3.5(h) of Executive Order 12333 of December 4, 1981 (United States Intelligence Activities), as amended. (t) The term “machine learning” means a set of techniques that can be used to train AI algorithms to improve performance at a task based on data. (u) The term “model weight” means a numerical parameter within an AI model that helps determine the model’s outputs in response to inputs. (v) The term “national security system” has the meaning set forth in 44 U.S.C. 3552(b)(6). (w) The\n"
     ]
    }
   ],
   "source": [
    "query = \"How does the document define machine learning?\"\n",
    "\n",
    "# Find the closest match(es) between the question bassed in and the embedded documents using Euclidena Distance.\n",
    "nearest_neighbor = faiss_vector_engine.nearestNeighbor(search_statement=query, limit = 3, insight_id = server_connection.cur_insight)\n",
    "\n",
    "for index, result in enumerate(nearest_neighbor):\n",
    "    print(f\"Result {index + 1}\")\n",
    "    print(f\"SCORE: {result['Score']}\")\n",
    "    print(f\"TOKENS: {result['Tokens']}\")\n",
    "    print(f\"CONTENT: {result['Content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'fileName': 'constitution.pdf', 'fileSize': 404.2470703125, 'lastModified': '2025-03-21 16:18:35'}]\n"
     ]
    }
   ],
   "source": [
    "# Names of the files we want to remove\n",
    "file_names = ['ai-whitehouse.pdf']\n",
    "\n",
    "faiss_vector_engine.removeDocument(file_names = file_names)\n",
    "\n",
    "print(faiss_vector_engine.listDocuments())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DatabaseEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     LOCATION HEIGHT WEIGHT\n",
      "0  Buckingham     61    256\n",
      "1         Bjc     60    220\n",
      "2      Louisa     61    203\n",
      "3      Louisa     59    204\n",
      "4      Louisa     61    220\n",
      "5      Louisa     58    210\n"
     ]
    }
   ],
   "source": [
    "from ai_server import DatabaseEngine\n",
    "\n",
    "# An example H2 diabetes database\n",
    "db_engine_id = os.getenv('DEV_DATABASE_ENGINE_ID')\n",
    "\n",
    "# Connect to the database by passing the engine id and the current insight id\n",
    "db = DatabaseEngine(engine_id=db_engine_id, insight_id=server_connection.cur_insight)\n",
    "\n",
    "query = db.execQuery(query = \"SELECT height, weight, location FROM diabetes WHERE height < 62 AND weight > 200\")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  LOCATION HEIGHT WEIGHT\n",
      "0  Rosslyn     65    200\n"
     ]
    }
   ],
   "source": [
    "insert = \"INSERT INTO diabetes (height, weight, location) VALUES (65, 200, 'Rosslyn')\"\n",
    "\n",
    "# NOTE you will need to be an author or editor of the database to insert or delete data\n",
    "db.insertData(query = insert)\n",
    "\n",
    "query = db.execQuery(query = \"SELECT height, weight, location FROM diabetes WHERE location = 'Rosslyn'\")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "remove = \"DELETE FROM diabetes WHERE location = 'Rosslyn'\"\n",
    "\n",
    "db.removeData(query = remove)\n",
    "\n",
    "query = db.execQuery(query = \"SELECT height, weight, location FROM diabetes WHERE location = 'Rosslyn'\")\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FunctionEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_server import FunctionEngine\n",
    "\n",
    "# Weather function engine\n",
    "weather_id = os.getenv('DEV_FUNCTION_ENGINE_ID')\n",
    "\n",
    "function = FunctionEngine(engine_id=weather_id, insight_id=server_connection.cur_insight)\n",
    "\n",
    "# Parameters will change based on the function you are using\n",
    "output = function.execute({\"lat\":\"37.540\",\"lon\":\"77.4360\"})\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StorageEngine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listing Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- machine-readable-business-employment-data-mar-2024-quarter.csv\n",
      "-- version/\n",
      "-- {'Path': 'machine-readable-business-employment-data-mar-2024-quarter.csv', 'Name': 'machine-readable-business-employment-data-mar-2024-quarter.csv', 'Size': 3583192.0, 'MimeType': 'text/csv; charset=utf-8', 'ModTime': '2024-06-13T19:11:49.768746813Z', 'IsDir': False, 'Tier': 'STANDARD', 'Metadata': {'atime': '2024-06-13T19:11:49.768746813Z', 'btime': '2024-11-01T21:28:04Z', 'content-type': 'text/csv; charset=utf-8', 'gid': '0', 'mode': '100640', 'mtime': '2024-06-13T19:11:49.768746813Z', 'tier': 'STANDARD', 'uid': '0'}}\n",
      "-- {'Path': 'version', 'Name': 'version', 'Size': 0.0, 'MimeType': 'inode/directory', 'ModTime': '2000-01-01T00:00:00.000000000Z', 'IsDir': True}\n"
     ]
    }
   ],
   "source": [
    "from ai_server import StorageEngine\n",
    "\n",
    "# Example S3 bucket\n",
    "storage_engine_id = os.getenv('DEV_STORAGE_ENGINE_ID')\n",
    "\n",
    "storageEngine = StorageEngine(engine_id = storage_engine_id, insight_id = server_connection.cur_insight)\n",
    "\n",
    "s3_storage_path = \"/my-new-test-folder/\"\n",
    "\n",
    "# A list of the files in the given path\n",
    "my_dir = storageEngine.list(storagePath = s3_storage_path)\n",
    "\n",
    "for file in my_dir:\n",
    "    print (f\"-- {file}\")\n",
    "\n",
    "# A list of details about the files in the given path\n",
    "my_dir_details = storageEngine.listDetails(storagePath = '/my-new-test-folder/')\n",
    "\n",
    "for file_details in my_dir_details:\n",
    "    print (f\"-- {file_details}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Response --\n",
      "2\n",
      "Full Response --\n",
      "{'insightID': 'e69849f5-8956-494b-b6ce-596e7d6259c4', 'pixelReturn': [{'pixelId': '20', 'pixelExpression': '1 + 1 ;', 'isMeta': False, 'timeToRun': 1, 'output': 2, 'operationType': ['OPERATION']}]}\n"
     ]
    }
   ],
   "source": [
    "# You can use the active server connection to run pixels\n",
    "\n",
    "simple_pixel_response = server_connection.run_pixel('1+1')\n",
    "\n",
    "print('Simple Response --')\n",
    "print(simple_pixel_response)\n",
    "\n",
    "full_pixel_response = server_connection.run_pixel('1+1', full_response=True)\n",
    "\n",
    "print('Full Response --')\n",
    "print(full_pixel_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chat with pixels example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'numberOfTokensInResponse': 6, 'numberOfTokensInPrompt': 554, 'response': 'The capital of Connecticut is Hartford.', 'messageId': 'a9278cbf-1470-4c40-882c-7adf8cae9cdd', 'roomId': 'e69849f5-8956-494b-b6ce-596e7d6259c4'}\n"
     ]
    }
   ],
   "source": [
    "engine_id = os.getenv('DEV_LLM_CHAT_ENGINE_ID')\n",
    "\n",
    "chat = server_connection.run_pixel(\n",
    "    f\"LLM ( engine = [ '{engine_id}' ] , command = [ '<encode>What is the capital of Connecticut</encode>' ] , paramValues = [ {{ 'max_new_tokens' : 200 , 'temperature' : 0.3 }} ] )\"\n",
    ")\n",
    "print(chat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
